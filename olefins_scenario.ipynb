{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:08:57.124516Z",
     "start_time": "2026-02-13T15:08:57.115856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n"
   ],
   "id": "a97d5588dc5b445e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:08:58.582666Z",
     "start_time": "2026-02-13T15:08:58.569394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# OLEFINS — Runner piloté par olefins_config.yaml\n",
    "# =============================================================================\n",
    "\n",
    "CONFIG_PATH = Path(\"config_olefins.yaml\")  # <-- adapter si besoin\n",
    "\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Paramètres YAML\n",
    "# -----------------------------\n",
    "YEAR_START = int(CFG[\"years\"][\"start\"])\n",
    "YEAR_END = int(CFG[\"years\"][\"end\"])\n",
    "YEARS = np.arange(YEAR_START, YEAR_END + 1)\n",
    "\n",
    "INP = CFG[\"input\"]\n",
    "OLEFINS_CSV = Path(INP[\"csv_path\"])\n",
    "BASE_YEAR = int(INP[\"base_year\"])\n",
    "COUNTRIES_SEL = INP.get(\"countries\", \"ALL\")\n",
    "ACTIVITY_COL_CONF = str(INP.get(\"activity_column\", \"olefins_production\")).strip()\n",
    "\n",
    "OUT = CFG[\"output\"]\n",
    "OUT_BASE_DIR = Path(OUT[\"base_dir\"])\n",
    "SCEN_DIRS = {k: OUT_BASE_DIR / v for k, v in OUT[\"scenario_dirs\"].items()}\n",
    "OUT_NAME = str(OUT.get(\"output_csv_name\", \"olefins.csv\"))\n",
    "SCOPE_TAG = str(OUT.get(\"scope\", \"petrochem_olefins\"))\n",
    "\n",
    "MAP = CFG.get(\"scenario_mapping\", {}).get(\"refinery_to_olefins\", {})\n",
    "LINK = CFG.get(\"linkage\", {})\n",
    "LINK_ENABLED = bool(LINK.get(\"enabled\", False))\n",
    "\n",
    "H2M = CFG[\"h2_demand_model\"]\n",
    "H2_INT_2019 = float(H2M[\"intensity_2019_tH2_per_activity_unit\"])\n",
    "\n",
    "INT_TR = H2M[\"intensity_trajectory\"]\n",
    "INT_START = int(INT_TR[\"start\"])\n",
    "INT_END = int(INT_TR[\"end\"])\n",
    "INT_FACT_2050 = {k: float(v) for k, v in INT_TR[\"scenario_factors_2050\"].items()}\n",
    "\n",
    "ACT_TR = H2M[\"activity_trajectory_if_no_linkage\"]\n",
    "ACT_START = int(ACT_TR[\"start\"])\n",
    "ACT_END = int(ACT_TR[\"end\"])\n",
    "ACT_DECL_2050 = {k: float(v) for k, v in ACT_TR[\"scenario_decline_2050\"].items()}\n",
    "\n",
    "CHK = CFG.get(\"checks\", {})\n",
    "CHK_NONNEG = bool(CHK.get(\"non_negative\", True))\n",
    "CHK_COV = bool(CHK.get(\"full_year_coverage\", True))\n",
    "CHK_SCOPE = bool(CHK.get(\"enforce_scope_tag\", True))\n",
    "CHK_WARN_DIVERGE = bool(CHK.get(\"warn_if_diverges_from_refinery_index\", True))\n"
   ],
   "id": "c25864dca2fa7da1",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:03.405441Z",
     "start_time": "2026-02-13T15:09:03.391638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Utils\n",
    "# =============================================================================\n",
    "def linear_ramp(years: np.ndarray, start: int, end: int) -> np.ndarray:\n",
    "    if end <= start:\n",
    "        return (years >= start).astype(float)\n",
    "    x = (years - start) / (end - start)\n",
    "    return np.clip(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def check_coverage(df: pd.DataFrame, y0: int, y1: int, keys=(\"country\", \"scenario\")) -> None:\n",
    "    exp = y1 - y0 + 1\n",
    "    for k, g in df.groupby(list(keys)):\n",
    "        ys = np.sort(g[\"year\"].unique())\n",
    "        if ys[0] != y0 or ys[-1] != y1 or len(ys) != exp:\n",
    "            raise ValueError(\n",
    "                f\"Couverture années KO pour {k}: {ys[0]}..{ys[-1]} n={len(ys)} (attendu {exp}).\"\n",
    "            )\n",
    "\n",
    "\n",
    "def detect_activity_column(df: pd.DataFrame, preferred: str) -> str:\n",
    "    if preferred in df.columns:\n",
    "        return preferred\n",
    "    cands = [\n",
    "        c for c in df.columns\n",
    "        if isinstance(c, str)\n",
    "        and (\"olefins\" in c.lower())\n",
    "        and ((\"production\" in c.lower()) or (\"activity\" in c.lower()))\n",
    "    ]\n",
    "    if len(cands) == 1:\n",
    "        return cands[0]\n",
    "    if len(cands) > 1:\n",
    "        raise ValueError(f\"Colonnes d'activité ambiguës: {cands} (précisez input.activity_column).\")\n",
    "    raise ValueError(f\"Impossible d'identifier la colonne d'activité. Colonnes: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "def read_olefins_raw(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    if \"country\" not in df.columns or \"year\" not in df.columns:\n",
    "        raise ValueError(f\"{path}: colonnes requises manquantes (attendu au moins country, year).\")\n",
    "    df = df.copy()\n",
    "    df[\"country\"] = df[\"country\"].astype(str).str.strip().str.upper()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"year\"])\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_base_activity(\n",
    "    df_in: pd.DataFrame,\n",
    "    base_year: int,\n",
    "    activity_col: str,\n",
    "    scenario_col: str | None = \"scenario\",\n",
    "    dedup_policy: str = \"prefer_DE\",\n",
    ") -> pd.DataFrame:\n",
    "    df = df_in[df_in[\"year\"] == base_year].copy()\n",
    "    if df.empty:\n",
    "        years = sorted(df_in[\"year\"].unique().tolist())\n",
    "        raise ValueError(f\"Aucune ligne pour base_year={base_year}. Années dispo: {years[:10]}...\")\n",
    "\n",
    "    df[activity_col] = pd.to_numeric(df[activity_col], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[activity_col])\n",
    "\n",
    "    if CHK_NONNEG and (df[activity_col] < -1e-9).any():\n",
    "        raise ValueError(\"Activité négative détectée en année de base.\")\n",
    "\n",
    "    # Option: si scenario existe, prioriser 'DE' (main) pour l'ancrage\n",
    "    if scenario_col and scenario_col in df.columns and dedup_policy == \"prefer_DE\":\n",
    "        df_s = df[df[scenario_col].astype(str).str.strip().str.upper() == \"DE\"]\n",
    "        if not df_s.empty:\n",
    "            df = df_s\n",
    "\n",
    "    base = df.groupby(\"country\", as_index=False)[activity_col].mean()\n",
    "    base = base.rename(columns={activity_col: \"activity_2019\"})\n",
    "    return base\n",
    "\n",
    "\n",
    "# ---- linkage refinery -> index(t) ----\n",
    "def _refinery_csv_path(\n",
    "    refinery_outputs_base_dir: Path,\n",
    "    refinery_scenario: str,\n",
    "    refinery_file: str,\n",
    "    scenario_dirs: dict[str, str] | None = None,\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Résout le chemin du CSV refinery pour un scenario logique (ex: 'more-molecule').\n",
    "\n",
    "    Supporte 2 conventions :\n",
    "    - défaut : <base_dir>/<refinery_scenario>/<refinery_file>\n",
    "    - si mapping YAML fourni : <base_dir>/<scenario_dirs[refinery_scenario]>/<refinery_file>\n",
    "    \"\"\"\n",
    "    if scenario_dirs and refinery_scenario in scenario_dirs:\n",
    "        return refinery_outputs_base_dir / scenario_dirs[refinery_scenario] / refinery_file\n",
    "    return refinery_outputs_base_dir / refinery_scenario / refinery_file\n",
    "\n",
    "\n",
    "def read_refinery_activity(refinery_csv: Path, activity_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(refinery_csv)\n",
    "    needed = {\"country\", \"scenario\", \"year\", activity_col}\n",
    "    missing = needed.difference(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{refinery_csv}: colonnes manquantes {sorted(missing)}. Colonnes: {list(df.columns)}\")\n",
    "\n",
    "    out = df[[\"country\", \"scenario\", \"year\", activity_col]].copy()\n",
    "    out[\"country\"] = out[\"country\"].astype(str).str.strip().str.upper()\n",
    "    out[\"scenario\"] = out[\"scenario\"].astype(str).str.strip()\n",
    "    out[\"year\"] = pd.to_numeric(out[\"year\"], errors=\"coerce\").astype(int)\n",
    "    out[activity_col] = pd.to_numeric(out[activity_col], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    # si le fichier est détaillé par techno, on somme\n",
    "    out = out.groupby([\"country\", \"scenario\", \"year\"], as_index=False)[activity_col].sum()\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_refinery_index(\n",
    "    refinery_df: pd.DataFrame,\n",
    "    refinery_scenario: str,\n",
    "    activity_col: str,\n",
    "    base_year: int,\n",
    "    years: np.ndarray,\n",
    ") -> pd.DataFrame:\n",
    "    df = refinery_df[refinery_df[\"scenario\"] == refinery_scenario].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"Aucune donnée refinery pour scenario='{refinery_scenario}'.\")\n",
    "\n",
    "    rows = []\n",
    "    for c, g in df.groupby(\"country\"):\n",
    "        s = g.set_index(\"year\")[activity_col].sort_index().reindex(years)\n",
    "        s = s.interpolate(limit_direction=\"both\").fillna(0.0)\n",
    "\n",
    "        base = float(s.loc[base_year]) if base_year in s.index else 0.0\n",
    "        if abs(base) < 1e-12:\n",
    "            idx = np.ones_like(years, dtype=float)\n",
    "        else:\n",
    "            idx = s.to_numpy() / base\n",
    "\n",
    "        rows.append(pd.DataFrame({\n",
    "            \"country\": c,\n",
    "            \"year\": years,\n",
    "            \"refinery_scenario\": refinery_scenario,\n",
    "            \"refinery_activity\": s.to_numpy(),\n",
    "            \"refinery_index\": idx,\n",
    "        }))\n",
    "\n",
    "    out = pd.concat(rows, ignore_index=True)\n",
    "    if CHK_NONNEG and (out[\"refinery_index\"] < -1e-9).any():\n",
    "        raise ValueError(\"Index refinery négatif détecté.\")\n",
    "    return out\n"
   ],
   "id": "d9c98dedd64fcaec",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:09.935456Z",
     "start_time": "2026-02-13T15:09:09.914699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 1) Lecture olefins + base 2019\n",
    "# =============================================================================\n",
    "df_ole = read_olefins_raw(OLEFINS_CSV)\n",
    "\n",
    "# pays\n",
    "if COUNTRIES_SEL == \"ALL\":\n",
    "    COUNTRIES = sorted(df_ole.loc[df_ole[\"year\"] == BASE_YEAR, \"country\"].unique().tolist())\n",
    "else:\n",
    "    COUNTRIES = [str(c).strip().upper() for c in COUNTRIES_SEL]\n",
    "\n",
    "df_ole = df_ole[df_ole[\"country\"].isin(COUNTRIES)].copy()\n",
    "\n",
    "ACT_COL = detect_activity_column(df_ole, preferred=ACTIVITY_COL_CONF)\n",
    "base = build_base_activity(df_ole, base_year=BASE_YEAR, activity_col=ACT_COL, scenario_col=\"scenario\", dedup_policy=\"prefer_DE\")\n",
    "\n",
    "if base.empty:\n",
    "    raise ValueError(\"Base olefins vide (après filtres).\")\n",
    "\n",
    "base_map = dict(zip(base[\"country\"], base[\"activity_2019\"]))\n"
   ],
   "id": "a48b8e868fbd40c6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:12.792367Z",
     "start_time": "2026-02-13T15:09:12.545230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 2) Index refinery (si activé) : un index par scénario olefins\n",
    "# =============================================================================\n",
    "refinery_index_by_ole_sc: dict[str, pd.DataFrame] = {}\n",
    "refinery_index_map: dict[tuple[str, str, int], float] = {}\n",
    "\n",
    "if LINK_ENABLED:\n",
    "    ref_base = Path(LINK[\"refinery_outputs_base_dir\"])\n",
    "    ref_file = str(LINK.get(\"refinery_output_csv_name\", \"refinery.csv\"))\n",
    "    ref_act_col = str(LINK.get(\"refinery_activity_column\", \"refinery_output_total\"))\n",
    "    ref_base_year = int(LINK.get(\"base_year\", BASE_YEAR))\n",
    "    missing_policy = str(LINK.get(\"missing_country_policy\", \"keep_base\")).strip()\n",
    "\n",
    "    # quels scénarios refinery doivent être chargés ?\n",
    "    needed_ref_scenarios = sorted(set(MAP.keys()))\n",
    "    if not needed_ref_scenarios:\n",
    "        raise ValueError(\"scenario_mapping.refinery_to_olefins est vide : impossible de lier à refinery.\")\n",
    "\n",
    "    # lecture + concat\n",
    "    ref_all = []\n",
    "    ref_scenario_dirs = LINK.get(\"refinery_scenario_dirs\", None)\n",
    "    for ref_sc in needed_ref_scenarios:\n",
    "        pth = _refinery_csv_path(\n",
    "            ref_base,\n",
    "            refinery_scenario=ref_sc,\n",
    "            refinery_file=ref_file,\n",
    "            scenario_dirs=ref_scenario_dirs,\n",
    "        )\n",
    "        if not pth.exists():\n",
    "            raise FileNotFoundError(f\"Refinery CSV introuvable: {pth}\")\n",
    "        ref_all.append(read_refinery_activity(pth, activity_col=ref_act_col))\n",
    "    ref_all = pd.concat(ref_all, ignore_index=True)\n",
    "\n",
    "    # index par scénario olefins (via mapping)\n",
    "    for ref_sc, ole_sc in MAP.items():\n",
    "        idx_df = build_refinery_index(ref_all, refinery_scenario=ref_sc, activity_col=ref_act_col,\n",
    "                                      base_year=ref_base_year, years=YEARS)\n",
    "        refinery_index_by_ole_sc[ole_sc] = idx_df\n",
    "\n",
    "        # map pour lookup rapide\n",
    "        for _, r in idx_df.iterrows():\n",
    "            refinery_index_map[(ole_sc, r[\"country\"], int(r[\"year\"]))] = float(r[\"refinery_index\"])\n",
    "\n",
    "    # policy missing country\n",
    "    def get_index(ole_sc: str, country: str, year: int) -> float:\n",
    "        key = (ole_sc, country, int(year))\n",
    "        if key in refinery_index_map:\n",
    "            return refinery_index_map[key]\n",
    "        if missing_policy == \"keep_base\":\n",
    "            return 1.0\n",
    "        raise ValueError(f\"Pays manquant dans l'index refinery pour {ole_sc}/{country}/{year}.\")"
   ],
   "id": "3bd154e2ebd7bf91",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:15.435945Z",
     "start_time": "2026-02-13T15:09:15.415281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 3) Scénarios olefins : activité(t) + intensité(t) => h2_demand(t)\n",
    "# =============================================================================\n",
    "def intensity_series(ole_sc: str) -> np.ndarray:\n",
    "    if ole_sc not in INT_FACT_2050:\n",
    "        raise ValueError(f\"Facteur intensité 2050 manquant pour scénario '{ole_sc}' dans YAML.\")\n",
    "    f2050 = float(INT_FACT_2050[ole_sc])\n",
    "    r = linear_ramp(YEARS, INT_START, INT_END)\n",
    "    return H2_INT_2019 * (1.0 + r * (f2050 - 1.0))\n",
    "\n",
    "\n",
    "def activity_series(country: str, ole_sc: str) -> np.ndarray:\n",
    "    a0 = float(base_map[country])\n",
    "\n",
    "    if LINK_ENABLED:\n",
    "        idx = np.array([get_index(ole_sc, country, int(y)) for y in YEARS], dtype=float)\n",
    "        if CHK_WARN_DIVERGE:\n",
    "            # warning simple : si l'index explose, c'est suspect (audit)\n",
    "            if np.nanmax(idx) > 5.0:\n",
    "                print(f\"[WARN] Index refinery très élevé pour {ole_sc}/{country}: max={np.nanmax(idx):.2f}\")\n",
    "        return a0 * idx\n",
    "\n",
    "    # fallback autonome si linkage désactivé\n",
    "    if ole_sc not in ACT_DECL_2050:\n",
    "        raise ValueError(f\"Decline 2050 manquant pour '{ole_sc}' (activity_trajectory_if_no_linkage).\")\n",
    "    d2050 = float(ACT_DECL_2050[ole_sc])\n",
    "    r = linear_ramp(YEARS, ACT_START, ACT_END)\n",
    "    factor = 1.0 - r * d2050\n",
    "    return a0 * factor\n",
    "\n",
    "\n",
    "rows = []\n",
    "for ole_sc in SCEN_DIRS.keys():\n",
    "    h2_int = intensity_series(ole_sc)\n",
    "\n",
    "    for c in COUNTRIES:\n",
    "        act = activity_series(c, ole_sc)\n",
    "        h2 = act * h2_int\n",
    "\n",
    "        df_c = pd.DataFrame({\n",
    "            \"country\": c,\n",
    "            \"scenario\": ole_sc,\n",
    "            \"year\": YEARS,\n",
    "            \"olefins_activity\": act,\n",
    "            \"h2_intensity\": h2_int,\n",
    "            \"h2_demand_t_per_yr\": h2,\n",
    "            \"scope\": SCOPE_TAG,\n",
    "        })\n",
    "        rows.append(df_c)\n",
    "\n",
    "df_out = pd.concat(rows, ignore_index=True)"
   ],
   "id": "507aa2b2bd7bc39f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:18.033517Z",
     "start_time": "2026-02-13T15:09:18.010503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 4) Checks / invariants\n",
    "# =============================================================================\n",
    "if CHK_SCOPE:\n",
    "    if df_out[\"scope\"].nunique() != 1 or df_out[\"scope\"].iloc[0] != SCOPE_TAG:\n",
    "        raise ValueError(f\"Scope invalide : attendu '{SCOPE_TAG}'.\")\n",
    "\n",
    "if CHK_NONNEG:\n",
    "    if (df_out[[\"olefins_activity\", \"h2_demand_t_per_yr\"]] < -1e-9).any(axis=None):\n",
    "        raise ValueError(\"Valeurs négatives détectées (activité ou demande H2).\")\n",
    "\n",
    "if CHK_COV:\n",
    "    check_coverage(df_out, YEAR_START, YEAR_END, keys=(\"country\", \"scenario\"))\n"
   ],
   "id": "5b2af46c13713602",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T15:09:19.797555Z",
     "start_time": "2026-02-13T15:09:19.769300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# 5) Écriture sur disque : un dossier par scénario\n",
    "# =============================================================================\n",
    "OUT_BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# trace base\n",
    "(base.sort_values(\"country\")).to_csv(OUT_BASE_DIR / \"olefins_base_2019.csv\", index=False)\n",
    "\n",
    "# trace index refinery (si linkage)\n",
    "if LINK_ENABLED:\n",
    "    idx_trace = pd.concat(\n",
    "        [v.assign(olefins_scenario=k) for k, v in refinery_index_by_ole_sc.items()],\n",
    "        ignore_index=True\n",
    "    )\n",
    "    idx_trace.to_csv(OUT_BASE_DIR / \"olefins_refinery_index.csv\", index=False)\n",
    "\n",
    "# outputs\n",
    "for ole_sc, out_dir in SCEN_DIRS.items():\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df_sc = df_out[df_out[\"scenario\"] == ole_sc].copy()\n",
    "    df_sc.to_csv(out_dir / OUT_NAME, index=False)\n",
    "\n",
    "# metadata (traçabilité)\n",
    "meta = {\n",
    "    \"config_path\": str(CONFIG_PATH),\n",
    "    \"olefins_input_csv\": str(OLEFINS_CSV),\n",
    "    \"activity_column_used\": ACT_COL,\n",
    "    \"countries_n\": len(COUNTRIES),\n",
    "    \"years\": {\"start\": YEAR_START, \"end\": YEAR_END},\n",
    "    \"linkage_enabled\": LINK_ENABLED,\n",
    "    \"scope\": SCOPE_TAG,\n",
    "    \"scenario_dirs\": {k: str(v) for k, v in SCEN_DIRS.items()},\n",
    "}\n",
    "with (OUT_BASE_DIR / \"metadata.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"OK: scénarios olefins écrits dans\", OUT_BASE_DIR.resolve())\n",
    "for ole_sc, out_dir in SCEN_DIRS.items():\n",
    "    print(\" -\", ole_sc, \"->\", (out_dir / OUT_NAME).resolve())"
   ],
   "id": "3b630b97a354442d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: scénarios olefins écrits dans /Users/simonbrigode/Desktop/tp_pommes_kraft/data_country/industry/pommes-h2-network/data-pommes/scenarios-olefins\n",
      " - bau-eu -> /Users/simonbrigode/Desktop/tp_pommes_kraft/data_country/industry/pommes-h2-network/data-pommes/scenarios-olefins/bau-eu/olefins.csv\n",
      " - decarbo-eu -> /Users/simonbrigode/Desktop/tp_pommes_kraft/data_country/industry/pommes-h2-network/data-pommes/scenarios-olefins/decarbo-eu/olefins.csv\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
