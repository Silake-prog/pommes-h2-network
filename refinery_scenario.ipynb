{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-13T13:32:32.181676Z",
     "start_time": "2026-02-13T13:32:32.176072Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "CONFIG_PATH = Path(\"refinery_config.yaml\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T13:32:33.502556Z",
     "start_time": "2026-02-13T13:32:33.493481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def read_refinery_output(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # mode \"sans header\"\n",
    "    if not {\"country\", \"year\", \"refinery_output\"}.issubset(df.columns):\n",
    "        df = pd.read_csv(\n",
    "            path,\n",
    "            header=None,\n",
    "            names=[\"country\", \"year\", \"scenario\", \"refinery_output\"],\n",
    "        )\n",
    "\n",
    "    # normalisation noms de colonnes : scenario -> base_scenario\n",
    "    if \"base_scenario\" not in df.columns and \"scenario\" in df.columns:\n",
    "        df = df.rename(columns={\"scenario\": \"base_scenario\"})\n",
    "\n",
    "    needed = {\"country\", \"base_scenario\", \"year\", \"refinery_output\"}\n",
    "    if not needed.issubset(df.columns):\n",
    "        raise ValueError(f\"Colonnes attendues {sorted(needed)} ; trouvé {sorted(df.columns)}\")\n",
    "\n",
    "    df[\"country\"] = df[\"country\"].astype(str).str.strip().str.upper()\n",
    "    df[\"base_scenario\"] = df[\"base_scenario\"].astype(str).str.strip()\n",
    "    df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    df[\"refinery_output\"] = pd.to_numeric(df[\"refinery_output\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"year\", \"refinery_output\", \"base_scenario\"])\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "    df[\"refinery_output\"] = df[\"refinery_output\"].astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def assert_non_negative(df: pd.DataFrame, cols: list[str]) -> None:\n",
    "    cols_present = [c for c in cols if c in df.columns]\n",
    "    if not cols_present:\n",
    "        return\n",
    "    if (df[cols_present] < -1e-9).any(axis=None):\n",
    "        raise ValueError(\"Valeurs négatives détectées:\\n\" + str(df[cols_present].min()))\n",
    "\n",
    "\n",
    "def check_coverage(df: pd.DataFrame, y0: int, y1: int) -> None:\n",
    "    for k, g in df.groupby([\"country\", \"base_scenario\", \"scenario\", \"unit\"]):\n",
    "        ys = np.sort(g[\"year\"].unique())\n",
    "        expected_n = (y1 - y0 + 1)\n",
    "        if ys[0] != y0 or ys[-1] != y1 or len(ys) != expected_n:\n",
    "            raise ValueError(\n",
    "                f\"Couverture années KO pour {k}: \"\n",
    "                f\"{ys[0]}..{ys[-1]} n={len(ys)} (attendu {expected_n}).\"\n",
    "            )\n",
    "def piecewise_linear_annual(years: np.ndarray, anchors: dict[int, float]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Interpolation linéaire annuelle à partir d'ancres {year: value}.\n",
    "    - Extrapolation \"flat\" en dehors [min_anchor, max_anchor].\n",
    "    \"\"\"\n",
    "    y = years.astype(int)\n",
    "    xs = np.array(sorted(anchors.keys()), dtype=int)\n",
    "    vs = np.array([anchors[int(x)] for x in xs], dtype=float)\n",
    "\n",
    "    out = np.interp(y, xs, vs)  # interp linéaire dans l'intervalle\n",
    "    out[y < xs.min()] = vs[0]   # flat avant première ancre\n",
    "    out[y > xs.max()] = vs[-1]  # flat après dernière ancre\n",
    "    return out"
   ],
   "id": "dd02a449f70a994c",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T13:32:36.064218Z",
     "start_time": "2026-02-13T13:32:36.025506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "with CONFIG_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "YEAR_START = int(CFG[\"years\"][\"start\"])\n",
    "YEAR_END = int(CFG[\"years\"][\"end\"])\n",
    "YEARS = np.arange(YEAR_START, YEAR_END + 1)\n",
    "\n",
    "INPUT_CSV = Path(CFG[\"input\"][\"csv_path\"])\n",
    "BASE_YEAR = int(CFG[\"input\"][\"base_year\"])\n",
    "BASE_SCENARIO = str(CFG[\"input\"][\"scenario\"]).strip()\n",
    "\n",
    "OUT_BASE = Path(CFG[\"output\"][\"base_dir\"])\n",
    "SCENARIO_DIRS = {k: OUT_BASE / v for k, v in CFG[\"output\"][\"scenario_dirs\"].items()}\n",
    "\n",
    "REF = CFG[\"refinery\"]\n",
    "INEFF = float(REF.get(\"inefficiency_share\", 0.0))\n",
    "if INEFF < 0:\n",
    "    raise ValueError(\"refinery.inefficiency_share doit être >= 0.\")\n",
    "\n",
    "CONCAWE = REF[\"scenarios\"]\n",
    "\n",
    "# 1) Lecture input + base 2019\n",
    "df_in = read_refinery_output(INPUT_CSV)\n",
    "\n",
    "if CFG[\"input\"][\"countries\"] == \"ALL\":\n",
    "    COUNTRIES = sorted(\n",
    "        df_in.loc[\n",
    "            (df_in[\"year\"] == BASE_YEAR) & (df_in[\"base_scenario\"] == BASE_SCENARIO),\n",
    "            \"country\"\n",
    "        ].unique().tolist()\n",
    "    )\n",
    "else:\n",
    "    COUNTRIES = [str(c).strip().upper() for c in CFG[\"input\"][\"countries\"]]\n",
    "\n",
    "df_base = df_in.loc[\n",
    "    (df_in[\"country\"].isin(COUNTRIES)) &\n",
    "    (df_in[\"base_scenario\"] == BASE_SCENARIO) &\n",
    "    (df_in[\"year\"] == BASE_YEAR)\n",
    "].copy()\n",
    "\n",
    "if df_base.empty:\n",
    "    raise ValueError(\n",
    "        f\"Aucune base trouvée pour year={BASE_YEAR} et base_scenario={BASE_SCENARIO}.\"\n",
    "    )\n",
    "\n",
    "# une ligne par pays (si doublons)\n",
    "df_base = (\n",
    "    df_base.sort_values([\"country\"])\n",
    "           .groupby([\"country\", \"base_scenario\"], as_index=False)\n",
    "           .head(1)\n",
    ")\n",
    "\n",
    "base_output = {\n",
    "    (r[\"country\"], r[\"base_scenario\"]): float(r[\"refinery_output\"])\n",
    "    for _, r in df_base.iterrows()\n",
    "}\n",
    "BASE_KEYS = sorted(base_output.keys())\n",
    "\n",
    "\n",
    "def build_refinery_scenario(scn_name: str, scn_payload: dict) -> pd.DataFrame:\n",
    "    units = scn_payload[\"units\"]\n",
    "    unit_names = list(units.keys())\n",
    "\n",
    "    # --- Total capacity anchors (pour info et contrôle) ---\n",
    "    # total_cap(y) = somme des utilized_capacity_mton des unités\n",
    "    anchor_years = sorted(next(iter(units.values()))[\"utilized_capacity_mton\"].keys())\n",
    "    total_cap_anchors = {}\n",
    "    for y in anchor_years:\n",
    "        total_cap_anchors[int(y)] = float(\n",
    "            sum(float(units[u][\"utilized_capacity_mton\"][int(y)]) for u in unit_names)\n",
    "        )\n",
    "\n",
    "    # --- Unit capacity annual series + shares annual series ---\n",
    "    cap_annual = {}\n",
    "    for u in unit_names:\n",
    "        anchors = {int(y): float(units[u][\"utilized_capacity_mton\"][int(y)]) for y in anchor_years}\n",
    "        cap_annual[u] = piecewise_linear_annual(YEARS, anchors)\n",
    "\n",
    "    total_cap_annual = np.zeros_like(YEARS, dtype=float)\n",
    "    for u in unit_names:\n",
    "        total_cap_annual += cap_annual[u]\n",
    "\n",
    "    # shares = cap_u / total_cap ; si total_cap=0 -> share=0\n",
    "    shares_annual = {}\n",
    "    for u in unit_names:\n",
    "        s = np.zeros_like(YEARS, dtype=float)\n",
    "        np.divide(cap_annual[u], total_cap_annual, out=s, where=(total_cap_annual > 0))\n",
    "        shares_annual[u] = s\n",
    "\n",
    "    # Option : appliquer aussi une baisse \"niveau total\" via ratio total_cap(t)/total_cap(2024)\n",
    "    # -> simple et cohérent avec Concawe (si on considère refinery_output comme \"feed proxy\").\n",
    "    # 2019..2024: flat car Concawe commence à 2024.\n",
    "    y_ref = min(anchor_years)  # typiquement 2024\n",
    "    cap_ref = float(total_cap_anchors[int(y_ref)])\n",
    "    if cap_ref <= 0:\n",
    "        raise ValueError(f\"Total capacity de référence <=0 pour {scn_name} (year={y_ref}).\")\n",
    "\n",
    "    level_factor = total_cap_annual / cap_ref  # ~1 à 2024\n",
    "    # avant 2024, on force à 1 (plateau) pour ne pas inventer une tendance pré-2024\n",
    "    level_factor[YEARS < y_ref] = 1.0\n",
    "    level_factor = np.clip(level_factor, 0.0, None)\n",
    "\n",
    "    # --- Build long output ---\n",
    "    out = []\n",
    "    for (country, base_sc) in BASE_KEYS:\n",
    "        base = base_output[(country, base_sc)]\n",
    "        total_feed = base * level_factor  # proxy \"refinery_output(t)\"\n",
    "\n",
    "        for u in unit_names:\n",
    "            spec_wt = float(units[u][\"spec_cons_wt\"])\n",
    "            feed_u = total_feed * shares_annual[u]\n",
    "            h2_u = feed_u * (spec_wt / 100.0) * (1.0 + INEFF)\n",
    "\n",
    "            out.append(pd.DataFrame({\n",
    "                \"country\": country,\n",
    "                \"base_scenario\": base_sc,\n",
    "                \"scenario\": scn_name,\n",
    "                \"unit\": u,\n",
    "                \"year\": YEARS,\n",
    "                \"refinery_output_total\": total_feed,\n",
    "                \"unit_share\": shares_annual[u],\n",
    "                \"unit_feed\": feed_u,\n",
    "                \"spec_cons_wt\": spec_wt,\n",
    "                \"h2_demand\": h2_u,\n",
    "            }))\n",
    "\n",
    "    df = pd.concat(out, ignore_index=True)\n",
    "\n",
    "    # contrôles\n",
    "    assert_non_negative(df, [\"refinery_output_total\", \"unit_share\", \"unit_feed\", \"h2_demand\", \"spec_cons_wt\"])\n",
    "    check_coverage(df, YEAR_START, YEAR_END)\n",
    "\n",
    "    # identité : somme unit_feed == refinery_output_total (tolérance numérique)\n",
    "    chk = (\n",
    "        df.groupby([\"country\", \"base_scenario\", \"scenario\", \"year\"], as_index=False)\n",
    "          .agg(total_feed=(\"refinery_output_total\", \"first\"), sum_unit_feed=(\"unit_feed\", \"sum\"))\n",
    "    )\n",
    "    err = (chk[\"total_feed\"] - chk[\"sum_unit_feed\"]).abs().max()\n",
    "    if err > 1e-6:\n",
    "        raise ValueError(f\"Incohérence allocation feed (max abs err={err}) pour {scn_name}.\")\n",
    "\n",
    "    return df"
   ],
   "id": "e753b0f95c39418c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T13:32:41.379661Z",
     "start_time": "2026-02-13T13:32:41.164607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_outputs(scn_name: str, df: pd.DataFrame) -> None:\n",
    "    out_dir = SCENARIO_DIRS[scn_name]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(out_dir / \"refinery_eu_demand.csv\", index=False)\n",
    "\n",
    "\n",
    "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for scn_name, payload in CONCAWE.items():\n",
    "    df_s = build_refinery_scenario(scn_name, payload)\n",
    "    write_outputs(scn_name, df_s)\n",
    "\n",
    "with (OUT_BASE / \"config_effective.yaml\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.safe_dump(CFG, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "print(\"OK: scénarios refinery-eu-demand générés dans\", OUT_BASE)"
   ],
   "id": "9c9daf5728ad1cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: scénarios refinery-eu-demand générés dans /Users/simonbrigode/Desktop/tp_pommes_kraft/data_country/industry/pommes-h2-network/data-pommes/scenario-refinery\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
